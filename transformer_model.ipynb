{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b38e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import lax,random,numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "# import haiku as hk\n",
    "\n",
    "import optax\n",
    "\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import functools\n",
    "from typing import Any,Callable,Sequence,Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64, 32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'variance'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m mha = MHA(\u001b[32m4\u001b[39m,\u001b[32m32\u001b[39m)\n\u001b[32m     74\u001b[39m key, key1 = random.split(key)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m params = \u001b[43mmha\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[32m     77\u001b[39m output = mha.apply(params, output)\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mMHA.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     55\u001b[39m q = \u001b[38;5;28mself\u001b[39m.Q(x)\n\u001b[32m     56\u001b[39m w = \u001b[38;5;28mself\u001b[39m.W(x)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvariance\u001b[49m(x)\n\u001b[32m     58\u001b[39m k = \u001b[38;5;28mself\u001b[39m.K(x)\n\u001b[32m     60\u001b[39m head_dim = \u001b[38;5;28mself\u001b[39m.embed_dim//\u001b[38;5;28mself\u001b[39m.num_heads\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chxyi\\anaconda3\\envs\\DiT\\Lib\\site-packages\\jax\\_src\\deprecations.py:55\u001b[39m, in \u001b[36mdeprecation_getattr.<locals>.getattr\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     53\u001b[39m   warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m     54\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'jax.numpy' has no attribute 'variance'"
     ]
    }
   ],
   "source": [
    "#input is CxHxW latent image, transform into patches of size Nx(P^2*C)\n",
    "\n",
    "def patchify(lat_img, p):\n",
    "    C, H, W = lat_img.shape\n",
    "    assert (H % p == 0 and W % p == 0)\n",
    "    \n",
    "    lat_img = lat_img.reshape(C, H//p, p, W//p, p)\n",
    "\n",
    "    lat_img = lat_img.transpose(1, 3, 2, 4, 0)\n",
    "\n",
    "    patches = lat_img.reshape(-1, p*p*C)\n",
    "    return patches\n",
    "\n",
    "inp = random.normal(random.PRNGKey(23), (3,4,32,32))\n",
    "\n",
    "pat = jax.vmap(patchify, in_axes=(0,None))(inp, 4)\n",
    "# pat = patchify(inp,4)\n",
    "# print(pat.shape)\n",
    "\n",
    "def batch_patchify(batch_lat_img, p):\n",
    "    return jax.vmap(patchify, in_axes=(0,None))(batch_lat_img,p)\n",
    "\n",
    "#TODO: Positional encoding\n",
    "class EmbedPatch(nn.Module):\n",
    "    # patchdim: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.layer = nn.Dense(self.embed_dim)\n",
    "\n",
    "    # @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "embed = EmbedPatch(embed_dim=32)\n",
    "key = random.PRNGKey(23)\n",
    "params = embed.init(key, pat)\n",
    "output = embed.apply(params, pat)\n",
    "print(output.shape)\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        assert self.embed_dim%self.num_heads == 0, \"embed_dim not divisible by num_heads\"\n",
    "        self.W = nn.Dense(self.embed_dim)\n",
    "        self.K = nn.Dense(self.embed_dim)\n",
    "        self.Q = nn.Dense(self.embed_dim)\n",
    "        self.W0 = nn.Dense(self.embed_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #Assume x has shape (Batches, Seq_len, embed_dim)\n",
    "        B,S,_ = x.shape\n",
    "        q = self.Q(x)\n",
    "        w = self.W(x)\n",
    "        # jnp.variance(x)\n",
    "        k = self.K(x)\n",
    "\n",
    "        head_dim = self.embed_dim//self.num_heads\n",
    "        multi_q = q.reshape(B,S,self.num_heads,head_dim).transpose(0,2,1,3)\n",
    "        multi_w = w.reshape(B,S,self.num_heads,head_dim).transpose(0,2,1,3)\n",
    "        multi_k = k.reshape(B,S,self.num_heads,head_dim).transpose(0,2,1,3)\n",
    "        \n",
    "        attention = jnp.matmul(multi_q, multi_k.transpose(0,1,3,2))/jnp.sqrt(head_dim)\n",
    "        attention = nn.softmax(attention,-1)\n",
    "        z = jnp.matmul(attention,multi_w)\n",
    "        multi_z = self.W0(z.transpose(0,2,1,3).reshape(B,S,self.embed_dim))\n",
    "\n",
    "        return multi_z\n",
    "    \n",
    "\n",
    "mha = MHA(4,32)\n",
    "key, key1 = random.split(key)\n",
    "params = mha.init(key1, output) \n",
    "print(output)\n",
    "output = mha.apply(params, output)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Test with MNIST\n",
    "\n",
    "def custom_collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    # print((transposed_data))\n",
    "\n",
    "    imgs = np.array(transposed_data[0])\n",
    "    imgs = imgs.reshape(imgs.shape[0],1,imgs.shape[1],imgs.shape[2])\n",
    "    labels = np.array(transposed_data[1])\n",
    "\n",
    "    # print(len(imgs))\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='./train_mnist',train=True, download=True,transform=lambda x:(np.array(x, dtype=np.float32)))\n",
    "test_dataset = MNIST(root='./test_mnist',train=False, download=True,transform=lambda x: np.ravel(np.array(x, dtype=np.float32)))\n",
    "# print(type(train_dataset))\n",
    "# print((train_dataset[0][0].shape))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate, drop_last=True)\n",
    "\n",
    "batch_data = (next(iter(train_loader)))\n",
    "# batch_data = next(iter(train_loader))\n",
    "# batch_data = next(iter(train_loader))\n",
    "print((batch_data[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedder = EmbedPatch(self.embed_dim)\n",
    "        self.mha = MHA(self.num_heads,self.embed_dim)\n",
    "        self.linear = nn.Dense(10)\n",
    "        self.mlp = nn.Dense(self.embed_dim)\n",
    "        self.layernorm = nn.LayerNorm()\n",
    "        # self.relu = nn.relu()\n",
    "\n",
    "    def __call__(self,x):\n",
    "        activation = x\n",
    "        activation = batch_patchify(activation, 7)\n",
    "        activation = self.embedder(activation)\n",
    "        activation = self.layernorm(activation)\n",
    "        activation = self.mha(activation)\n",
    "        activation = self.layernorm(activation)\n",
    "        activation = self.mlp(activation)\n",
    "        activation = nn.relu(activation)\n",
    "        activation = self.mha(activation)\n",
    "        # activation = self.layernorm(activation)\n",
    "        activation = activation.reshape(activation.shape[0], -1)\n",
    "        activation = self.linear(activation)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f01060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2491689\n",
      "0.2794252\n",
      "0.24887462\n",
      "0.23497777\n",
      "0.22202174\n",
      "0.21747606\n",
      "0.21461372\n",
      "0.21179025\n",
      "0.21887426\n",
      "0.20959555\n",
      "0.21154885\n",
      "0.20637286\n",
      "0.20592622\n",
      "0.21477\n",
      "0.21296501\n",
      "0.21111849\n",
      "0.2066931\n",
      "0.1921023\n",
      "0.19493876\n",
      "0.18349828\n",
      "0.20488504\n",
      "0.18718891\n",
      "0.18495299\n",
      "0.20155847\n",
      "0.22873612\n",
      "0.22798553\n",
      "0.20073119\n",
      "0.18396893\n",
      "0.17839997\n",
      "0.17764737\n",
      "0.16039579\n",
      "0.17898393\n",
      "0.18765707\n",
      "0.19703157\n",
      "0.17930885\n",
      "0.16762269\n",
      "0.17732841\n",
      "0.19357608\n",
      "0.20906101\n",
      "0.1821375\n",
      "0.16943093\n",
      "0.17747033\n",
      "0.16382714\n",
      "0.17245486\n",
      "0.18947083\n",
      "0.18907173\n",
      "0.21488492\n",
      "0.20388447\n",
      "0.1827116\n",
      "0.16281569\n",
      "0.17374888\n",
      "0.15885599\n",
      "0.1670336\n",
      "0.1507827\n",
      "0.14224575\n",
      "0.14994673\n",
      "0.15892018\n",
      "0.15349555\n",
      "0.14422388\n",
      "0.15266816\n",
      "0.17002921\n",
      "0.17464897\n",
      "0.18210593\n",
      "0.1579622\n",
      "0.14776433\n",
      "0.14705795\n",
      "0.13789193\n",
      "0.1412973\n",
      "0.16482149\n",
      "0.20081027\n",
      "0.18203034\n",
      "0.19205974\n",
      "0.16150193\n",
      "0.14283237\n",
      "0.143156\n",
      "0.13999854\n",
      "0.13022624\n",
      "0.1294763\n",
      "0.15009367\n",
      "0.13433163\n",
      "0.13568218\n",
      "0.13955028\n",
      "0.1428421\n",
      "0.12009948\n",
      "0.10765703\n",
      "0.1302225\n",
      "0.113560036\n",
      "0.12559192\n",
      "0.14373104\n",
      "0.16420786\n",
      "0.13334249\n",
      "0.1374217\n",
      "0.13494578\n",
      "0.116588645\n",
      "0.10736566\n",
      "0.12237229\n",
      "0.10666674\n",
      "0.09765639\n",
      "0.1060944\n",
      "0.096333705\n",
      "0.09174251\n",
      "0.10037126\n",
      "0.10831087\n",
      "0.08908321\n",
      "0.094963506\n",
      "0.083623365\n",
      "0.10175796\n",
      "0.09253665\n",
      "0.08826386\n",
      "0.09031285\n",
      "0.09053714\n",
      "0.081802264\n",
      "0.095912576\n",
      "0.10629703\n",
      "0.12749448\n",
      "0.11993914\n",
      "0.0976436\n",
      "0.07689253\n",
      "0.09086943\n",
      "0.081447564\n",
      "0.070716195\n",
      "0.08722078\n",
      "0.07734408\n",
      "0.06488503\n",
      "0.075364105\n",
      "0.06643059\n",
      "0.06954416\n",
      "0.07461375\n",
      "0.0732605\n",
      "0.07820891\n",
      "0.0726143\n",
      "0.0795908\n",
      "0.093581475\n",
      "0.0668994\n",
      "0.084827736\n",
      "0.08365665\n",
      "0.081254534\n",
      "0.08721907\n",
      "0.06483232\n",
      "0.0689291\n",
      "0.05400266\n",
      "0.07504018\n",
      "0.06991318\n",
      "0.06805681\n",
      "0.05834477\n",
      "0.059357364\n",
      "0.074309625\n",
      "0.052768886\n",
      "0.0633006\n",
      "0.05969545\n",
      "0.07548011\n",
      "0.05727012\n",
      "0.0374731\n",
      "0.064830564\n",
      "0.05467121\n",
      "0.09168225\n",
      "0.07320893\n",
      "0.08484979\n",
      "0.05641831\n",
      "0.048179887\n",
      "0.041753415\n",
      "0.048531912\n",
      "0.057418693\n",
      "0.048333135\n",
      "0.050496496\n",
      "0.053661373\n",
      "0.054172408\n",
      "0.0545349\n",
      "0.06339933\n",
      "0.053371705\n",
      "0.062125422\n",
      "0.074504755\n",
      "0.06648755\n",
      "0.06173245\n",
      "0.0577618\n",
      "0.06818512\n",
      "0.058760893\n",
      "0.048435923\n",
      "0.0594435\n",
      "0.0515318\n",
      "0.067386016\n",
      "0.065195695\n",
      "0.052324455\n",
      "0.03710061\n",
      "0.043653857\n",
      "0.049613096\n",
      "0.051818993\n",
      "0.039682716\n",
      "0.053238448\n",
      "0.059188128\n",
      "0.05728474\n",
      "0.040474392\n",
      "0.04617725\n",
      "0.055679407\n",
      "0.04419431\n",
      "0.048534214\n",
      "0.051494855\n",
      "0.043794554\n",
      "0.05677519\n",
      "0.042851847\n",
      "0.046357524\n",
      "0.061146785\n",
      "0.03886496\n",
      "0.03885798\n",
      "0.05375759\n",
      "0.054979336\n",
      "0.053087503\n",
      "0.0576494\n",
      "0.040996786\n",
      "0.045392122\n",
      "0.04188956\n",
      "0.032181907\n",
      "0.044541296\n",
      "0.042197734\n",
      "0.043991443\n",
      "0.041388273\n",
      "0.045708735\n",
      "0.05673867\n",
      "0.058761504\n",
      "0.034779448\n",
      "0.0386245\n",
      "0.06158756\n",
      "0.03971944\n",
      "0.033956345\n",
      "0.049934056\n",
      "0.040061977\n",
      "0.037670746\n",
      "0.027712086\n",
      "0.047721956\n",
      "0.04018077\n",
      "0.058451958\n",
      "0.05870846\n",
      "0.02645777\n",
      "0.04050585\n",
      "0.04309855\n",
      "0.04315588\n",
      "0.046947032\n",
      "0.034691844\n",
      "0.02651148\n",
      "0.058678415\n",
      "0.048345696\n",
      "0.042969853\n",
      "0.05227023\n",
      "0.047410328\n",
      "0.057594746\n",
      "0.043599952\n",
      "0.034418352\n",
      "0.031377744\n",
      "0.0558064\n",
      "0.03316244\n",
      "0.050784934\n",
      "0.034641065\n",
      "0.04481369\n",
      "0.049820613\n",
      "0.035121318\n",
      "0.040996548\n",
      "0.04217821\n",
      "0.03642856\n",
      "0.020343764\n",
      "0.04539904\n",
      "0.046212085\n",
      "0.042725872\n",
      "0.02567744\n",
      "0.030883724\n",
      "0.027471945\n",
      "0.037537206\n",
      "0.049675506\n",
      "0.042243194\n",
      "0.03291673\n",
      "0.027120188\n",
      "0.033150196\n",
      "0.036533695\n",
      "0.043548245\n",
      "0.03452143\n",
      "0.027717574\n",
      "0.032382432\n",
      "0.03671491\n",
      "0.012313219\n",
      "0.040544085\n",
      "0.03737807\n",
      "0.053214025\n",
      "0.033596255\n",
      "0.038906064\n",
      "0.031937137\n",
      "0.031406607\n",
      "0.030302977\n",
      "0.02115098\n",
      "0.02803952\n",
      "0.037387915\n",
      "0.03952556\n",
      "0.028123647\n",
      "0.04117319\n",
      "0.02459844\n",
      "0.05331966\n",
      "0.020741964\n",
      "0.027761703\n",
      "0.042305883\n",
      "0.059062373\n",
      "0.038322818\n",
      "0.044506885\n",
      "0.041252058\n",
      "0.025007373\n",
      "0.0264296\n",
      "0.04208253\n",
      "0.038421106\n",
      "0.03747921\n",
      "0.020177651\n",
      "0.030205602\n",
      "0.02806525\n",
      "0.029275332\n",
      "0.019052831\n",
      "0.019507997\n",
      "0.02169967\n",
      "0.0241817\n",
      "0.023812056\n",
      "0.028409172\n",
      "0.03945136\n",
      "0.019109378\n",
      "0.028992534\n",
      "0.023460219\n",
      "0.024080642\n",
      "0.030621579\n",
      "0.034682196\n",
      "0.038171437\n",
      "0.035403144\n",
      "0.036123734\n",
      "0.02965716\n",
      "0.032585263\n",
      "0.022047404\n",
      "0.03322592\n",
      "0.016286626\n",
      "0.0270211\n",
      "0.026561443\n",
      "0.034694612\n",
      "0.05303469\n",
      "0.030116517\n",
      "0.028609717\n",
      "0.042191982\n",
      "0.03251409\n",
      "0.044057544\n",
      "0.032680154\n",
      "0.031682868\n",
      "0.032732915\n",
      "0.027781462\n",
      "0.018661624\n",
      "0.024561262\n",
      "0.03070285\n",
      "0.026849682\n",
      "0.041355032\n",
      "0.021219814\n",
      "0.029114217\n",
      "0.037637785\n",
      "0.034207385\n",
      "0.030913323\n",
      "0.029745743\n",
      "0.036307108\n",
      "0.0414533\n",
      "0.03737369\n",
      "0.04062691\n",
      "0.023439862\n",
      "0.021087836\n",
      "0.03447912\n",
      "0.019103622\n",
      "0.029581545\n",
      "0.019085018\n",
      "0.023762494\n",
      "0.028492292\n",
      "0.023792675\n",
      "0.033979155\n",
      "0.041091148\n",
      "0.026368648\n",
      "0.01974303\n",
      "0.022052405\n",
      "0.027162656\n",
      "0.025555098\n",
      "0.043740213\n",
      "0.03210482\n",
      "0.032449424\n",
      "0.03970214\n",
      "0.026192779\n",
      "0.022929123\n",
      "0.021453159\n",
      "0.030370623\n",
      "0.028400322\n",
      "0.0268589\n",
      "0.046849135\n",
      "0.019377328\n",
      "0.023067651\n",
      "0.034605637\n",
      "0.029627204\n",
      "0.019457985\n",
      "0.019434894\n",
      "0.01908367\n",
      "0.030156016\n",
      "0.03380998\n",
      "0.020000614\n",
      "0.017633175\n",
      "0.03455836\n",
      "0.025875945\n",
      "0.039560962\n",
      "0.016686672\n",
      "0.031454302\n",
      "0.018413786\n",
      "0.029533321\n",
      "0.033040386\n",
      "0.0272587\n",
      "0.025537703\n",
      "0.029958844\n",
      "0.035606664\n",
      "0.02714826\n",
      "0.011012997\n",
      "0.034220662\n",
      "0.022940958\n",
      "0.020545742\n",
      "0.040711768\n",
      "0.02766716\n",
      "0.021824716\n",
      "0.029937262\n",
      "0.038596198\n",
      "0.03413192\n",
      "0.023153314\n",
      "0.040577494\n",
      "0.04744677\n",
      "0.029585823\n",
      "0.0172336\n",
      "0.01576512\n",
      "0.019108562\n",
      "0.030953078\n",
      "0.029182876\n",
      "0.03318354\n",
      "0.023601001\n",
      "0.04244641\n",
      "0.031739574\n",
      "0.01643805\n",
      "0.022405593\n",
      "0.017251324\n",
      "0.023694606\n",
      "0.04353105\n",
      "0.02108474\n",
      "0.023274884\n",
      "0.020452509\n",
      "0.020338152\n",
      "0.019244716\n",
      "0.018143749\n",
      "0.035426375\n",
      "0.04484882\n",
      "0.03440434\n",
      "0.017820105\n",
      "0.030678196\n",
      "0.025549317\n",
      "0.022199353\n",
      "0.020828623\n",
      "0.024034504\n",
      "0.013994725\n",
      "0.025726253\n",
      "0.020040812\n",
      "0.019649312\n",
      "0.024034088\n",
      "0.038134497\n",
      "0.02242913\n",
      "0.022243453\n",
      "0.026230628\n",
      "0.03380462\n",
      "0.038559604\n",
      "0.02703082\n",
      "0.031571705\n",
      "0.020328395\n",
      "0.020853195\n"
     ]
    }
   ],
   "source": [
    "from jax import grad, value_and_grad\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "model = Model(64,4)\n",
    "dummy = random.normal(key, (1,1,28,28))\n",
    "params = model.init(key, dummy)\n",
    "\n",
    "def cross_entropy_loss(params,imgs, labels):\n",
    "    # logits: (batch_size, num_classes)\n",
    "    # labels: (batch_size,) with class indices\n",
    "    # patches = patchify(imgs, 7)\n",
    "    # embed = EmbedPatch(embed_dim=28)\n",
    "    # key = random.PRNGKey(23)\n",
    "    # params = embed.init(key, patches)\n",
    "    # output = embed.apply(params, patches)\n",
    "    logits = model.apply(params,imgs)\n",
    "    # print(logits.shape)\n",
    "    log_probs = jax.nn.log_softmax(logits)  # (batch_size, num_classes)\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=logits.shape[-1])\n",
    "    loss = -jnp.sum(one_hot_labels * log_probs, axis=-1)  # (batch_size,)\n",
    "    return loss.mean()\n",
    "\n",
    "def loss(params, imgs, labels):\n",
    "    output = model.apply(params, imgs)\n",
    "    log_probs = jax.nn.log_softmax(output)\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return -jnp.mean(log_probs*one_hot_labels)\n",
    "\n",
    "def update(params, imgs, gt_labels, lr=0.5):\n",
    "    l, grads = value_and_grad(loss)(params,imgs,gt_labels)\n",
    "    return l, jax.tree.map(lambda p, g: p - lr*g, params, grads)\n",
    "\n",
    "for epoch in range((NUM_EPOCHS)):\n",
    "\n",
    "    for cnt, (imgs, labels) in enumerate(train_loader):\n",
    "        # gt_labels = jax.nn.one_hot(labels,len(MNIST.classes))\n",
    "        # print(imgs.shape)\n",
    "        l, params = update(params, imgs, labels)\n",
    "\n",
    "        # if cnt % 50 == 0:\n",
    "        print(l)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
