{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a22374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63eb569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]]\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "\n",
    "\n",
    "def init_MLP(parkey, layer_widths, scale = 0.01):\n",
    "    params = []\n",
    "    keys = jax.random.split(parkey, num=len(layer_widths)-1)\n",
    "    for in_width, out_width, key in zip(layer_widths[:-1],layer_widths[1:], keys):\n",
    "        wkey, bkey = jax.random.split(key)\n",
    "        params.append([scale*jax.random.normal(wkey, shape=(out_width,in_width)),scale*jax.random.normal(bkey, shape=(out_width))])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "MLP_params = init_MLP(key, [784,512,256,10])\n",
    "print(jax.tree.map(lambda x: x.shape, MLP_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7bda25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n"
     ]
    }
   ],
   "source": [
    "# @jax.jit\n",
    "def predict(params, x):\n",
    "    # print(\"hi\")\n",
    "    hidden_layers = params[:-1]\n",
    "\n",
    "    activation = x\n",
    "    for w, b in hidden_layers:\n",
    "        activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
    "\n",
    "    w_last, b_last = params[-1]\n",
    "\n",
    "    logits = jnp.dot(w_last, activation) + b_last\n",
    "\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "dummy_img_flat = np.random.randn(16, 784)\n",
    "\n",
    "prediction = jax.vmap(predict, in_axes=(None, 0))(MLP_params, dummy_img_flat)\n",
    "batch_predict = jax.vmap(predict, in_axes=(None, 0))\n",
    "# prediction = jax.vmap(predict, in_axes=(None, 1))(MLP_params, dummy_img_flat)\n",
    "print(prediction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1798d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    # print((transposed_data))\n",
    "\n",
    "    imgs = np.array(transposed_data[0])\n",
    "    labels = np.array(transposed_data[1])\n",
    "\n",
    "    # print(len(imgs))\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='./train_mnist',train=True, download=True,transform=lambda x: np.ravel(np.array(x, dtype=np.float32)))\n",
    "test_dataset = MNIST(root='./test_mnist',train=False, download=True,transform=lambda x: np.ravel(np.array(x, dtype=np.float32)))\n",
    "# print(type(train_dataset))\n",
    "# print((train_dataset[0][0].shape))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "batch_data = next(iter(train_loader))\n",
    "print(len(batch_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2ac5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23637092\n",
      "0.095004536\n",
      "0.057015408\n",
      "0.050588895\n",
      "0.038365215\n",
      "0.026843121\n",
      "0.031915046\n",
      "0.02219086\n",
      "0.026118347\n",
      "0.022080217\n"
     ]
    }
   ],
   "source": [
    "from jax import grad, value_and_grad\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "def loss(params, imgs, gt_labels):\n",
    "    output = batch_predict(params, imgs)\n",
    "    return -jnp.mean(output*gt_labels)\n",
    "\n",
    "def update(params, imgs, gt_labels, lr=0.01):\n",
    "    l, grads = value_and_grad(loss)(params,imgs,gt_labels)\n",
    "    return l, jax.tree.map(lambda p, g: p - lr*g, params, grads)\n",
    "\n",
    "for epoch in range((NUM_EPOCHS)):\n",
    "\n",
    "    for cnt, (imgs, labels) in enumerate(train_loader):\n",
    "        gt_labels = jax.nn.one_hot(labels,len(MNIST.classes))\n",
    "        l, MLP_params = update(MLP_params, imgs, gt_labels)\n",
    "\n",
    "        # if cnt % 50 == 0:\n",
    "        print(l)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
