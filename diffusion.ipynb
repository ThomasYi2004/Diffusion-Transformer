{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b71c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import lax,random,numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "# import haiku as hk\n",
    "\n",
    "import optax\n",
    "\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import functools\n",
    "from typing import Any,Callable,Sequence,Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c51e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 9.99880075e-01 9.99740243e-01 ... 4.11860819e-05\n",
      " 4.03623599e-05 3.95551142e-05]\n"
     ]
    }
   ],
   "source": [
    "#Define constants:\n",
    "\n",
    "T = 1000\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "\n",
    "beta_schedule = jax.numpy.linspace(beta_start, beta_end, T)\n",
    "\n",
    "alpha_schedule = 1-beta_schedule\n",
    "alpha_prefix_product = np.ones(T+1)\n",
    "for i in range(1,1001):\n",
    "    alpha_prefix_product[i] = alpha_prefix_product[i-1]*alpha_schedule[i]\n",
    "\n",
    "print(alpha_prefix_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fe729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input is CxHxW latent image, transform into patches of size Nx(P^2*C)\n",
    "\n",
    "class Patchify(nn.Module):\n",
    "    p: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, lat_img):\n",
    "        B, C, H, W = lat_img.shape\n",
    "        p = self.p\n",
    "        assert (H % p == 0 and W % p == 0)\n",
    "        \n",
    "        lat_img = lat_img.reshape(B, C, H//p, p, W//p, p)\n",
    "    \n",
    "        lat_img = lat_img.transpose(0, 2, 4, 3, 5, 1)\n",
    "    \n",
    "        patches = lat_img.reshape(B,-1, p*p*C)\n",
    "        return patches\n",
    "\n",
    "# inp = random.normal(random.PRNGKey(23), (3,4,32,32))\n",
    "\n",
    "# pat = jax.vmap(patchify, in_axes=(0,None))(inp, 4)\n",
    "# pat = patchify(inp,4)\n",
    "# print(pat.shape)\n",
    "\n",
    "# def batch_patchify(batch_lat_img, p):\n",
    "#     return jax.vmap(patchify, in_axes=(0,None))(batch_lat_img,p)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Positional encoding\n",
    "class EmbedPatch(nn.Module):\n",
    "    # patchdim: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.layer = nn.Dense(self.embed_dim)\n",
    "\n",
    "    # @nn.compact\n",
    "    def __call__(self, x_t, t):\n",
    "        return self.layer(x_t)\n",
    "\n",
    "#CHATGPT\n",
    "class sinusoidal_embedding(nn.Module):\n",
    "    dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, timesteps):\n",
    "        dim = self.dim\n",
    "        half_dim = dim // 2\n",
    "        freqs = jnp.exp(-jnp.arange(half_dim) * (jnp.log(10000.0) / (half_dim - 1)))\n",
    "        args = timesteps[:, None] * freqs[None]  # [batch, half_dim]\n",
    "        embedding = jnp.concatenate([jnp.sin(args), jnp.cos(args)], axis=-1)\n",
    "        return embedding\n",
    "\n",
    "sin = sinusoidal_embedding(dim = 32)\n",
    "key = random.PRNGKey(23)\n",
    "params = sin.init(key, jnp.array([1,2]))\n",
    "output = sin.apply(params, jnp.array([1,2]))\n",
    "print(output)\n",
    "    \n",
    "# embed = EmbedPatch(embed_dim=32)\n",
    "# key = random.PRNGKey(23)\n",
    "# params = embed.init(key, pat)\n",
    "# output = embed.apply(params, pat)\n",
    "# print(output.shape)\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        assert self.embed_dim%self.num_heads == 0, \"embed_dim not divisible by num_heads\"\n",
    "        self.W = nn.Dense(self.embed_dim)\n",
    "        self.K = nn.Dense(self.embed_dim)\n",
    "        self.Q = nn.Dense(self.embed_dim)\n",
    "        self.W0 = nn.Dense(self.embed_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #Assume x has shape (Batches, Seq_len, embed_dim)\n",
    "        B,S,_ = x.shape\n",
    "        q = self.Q(x)\n",
    "        w = self.W(x)\n",
    "        k = self.K(x)\n",
    "\n",
    "        head_dim = self.embed_dim//self.num_heads\n",
    "        multi_q = q.reshape(B,S,self.num_heads,head_dim).transpose(0,2,1,3)\n",
    "        multi_w = w.reshape(B,S,self.num_heads,head_dim).transpose(0,2,1,3)\n",
    "        multi_k = k.reshape(B,S,self.num_heads,head_dim).transpose(0,2,1,3)\n",
    "        \n",
    "        attention = jnp.matmul(multi_q, multi_k.transpose(0,1,3,2))/jnp.sqrt(head_dim)\n",
    "        attention = nn.softmax(attention,-1)\n",
    "        z = jnp.matmul(attention,multi_w)\n",
    "        multi_z = self.W0(z.transpose(0,2,1,3).reshape(B,S,self.embed_dim))\n",
    "\n",
    "        return multi_z\n",
    "\n",
    "class DiT_block(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.layernorm = nn.LayerNorm()\n",
    "        self.mha = MHA(num_heads = self.num_heads,embed_dim = self.embed_dim)\n",
    "        self.ffd = nn.Sequential([\n",
    "            nn.Dense(self.embed_dim * 4),\n",
    "            nn.relu,\n",
    "            nn.Dense(self.embed_dim),\n",
    "        ])\n",
    "        self.mlp = nn.Dense(6*self.embed_dim)\n",
    "        \n",
    "\n",
    "    def __call__(self, x_t, t_emb):\n",
    "        #x_t.shape: (B,Seq_len, Seq_size)\n",
    "        activation = x_t\n",
    "        alpha1,beta1,gamma1,alpha2,beta2,gamma2 = jnp.split(self.mlp(t_emb), 6, axis=1)\n",
    "        means = jnp.mean(activation, axis=-1)\n",
    "        variances = jnp.var(activation,axis=-1)\n",
    "        res = activation\n",
    "        # activation = self.layernorm(x_t)\n",
    "        #scale,shift\n",
    "        activation = (activation-means[:, :, None])/variances[:, :, None]\n",
    "        print(gamma1.shape)\n",
    "        activation = (activation*gamma1[:,None,:])+beta1[:,None,:]\n",
    "\n",
    "        activation = self.mha(activation)\n",
    "        activation = activation*alpha1[:,None,:]\n",
    "        activation = activation + res\n",
    "        res2 = activation\n",
    "        # activation = self.layernorm(activation)\n",
    "        means2 = jnp.mean(activation, axis=-1)\n",
    "        variances2 = jnp.var(activation,axis=-1)\n",
    "        #scale,shift\n",
    "        activation = (activation-means2[:, :, None])/variances2[:,:,None]\n",
    "        activation = (activation*gamma2[:,None,:])+beta2[:,None,:]\n",
    "        \n",
    "        activation = self.ffd(activation)\n",
    "        #scale\n",
    "        activation = activation*alpha2[:,None,:]\n",
    "        activation = activation+res2\n",
    "        return activation\n",
    "    \n",
    "\n",
    "# mha = MHA(4,32)\n",
    "# key, key1 = random.split(key)\n",
    "# params = mha.init(key1, output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bed864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS SECTION IS WRITTEN BY CHATGPT\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transforms (standard ImageNet preprocessing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "data_dir = \"/kaggle/input/imagenet100\"\n",
    "\n",
    "# Collect train shards\n",
    "train_folders = [f\"{data_dir}/train.X{i}\" for i in range(1, 5)]\n",
    "\n",
    "train_datasets = [\n",
    "    datasets.ImageFolder(root=folder, transform=transform) \n",
    "    for folder in train_folders\n",
    "]\n",
    "\n",
    "# Merge into one dataset\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = datasets.ImageFolder(root=f\"{data_dir}/val.X\", transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Quick check\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Train batch images: {images.shape}, labels: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images[1][0])\n",
    "patchify = Patchify(p=4)\n",
    "key = random.PRNGKey(23)\n",
    "images = jnp.array(images)\n",
    "params = patchify.init(key, images)\n",
    "output = patchify.apply(params, images)\n",
    "print(images.shape)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "    p: int\n",
    "    n: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.patchify = Patchify(self.p)\n",
    "        self.layernorm = nn.LayerNorm()\n",
    "        self.dit = DiT_block(num_heads = self.num_heads, embed_dim = self.embed_dim)\n",
    "        self.sin_embed = sinusoidal_embedding(self.embed_dim)\n",
    "        \n",
    "    def __call__(self, x_t, t):\n",
    "        time_embedding = self.sin_embed(t)\n",
    "        print(time_embedding.shape)\n",
    "        activation = self.patchify(x_t)  #shape: BxSeq_lenxSeq_size\n",
    "        for i in range(self.n):\n",
    "            activation = self.dit(activation, time_embedding)\n",
    "        activation = self.layernorm(activation)\n",
    "\n",
    "\n",
    "model = Model(12,48,4,1)\n",
    "print(jnp.array(labels)[:,None].shape)\n",
    "key = random.PRNGKey(23)\n",
    "params = model.init(key, jnp.array(images), jnp.array(labels))\n",
    "output = model.apply(params, jnp.array(images), jnp.array(labels))\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
